<p align="center">
  <strong>ğŸ›ï¸ Project Tractatus-Eval</strong><br>
  <em>A Benchmark for Spatial Embodied Logic in Large Language Models</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.8%2B-blue?style=flat-square" alt="Python">
  <img src="https://img.shields.io/badge/EleutherAI-lm--eval--harness-green?style=flat-square" alt="lm-eval-harness">
  <img src="https://img.shields.io/badge/license-MIT-lightgrey?style=flat-square" alt="License">
  <img src="https://img.shields.io/badge/samples-1000-orange?style=flat-square" alt="Samples">
</p>

<p align="center">
  <a href="#why-this-exists">English</a> | <a href="#ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªé¡¹ç›®">ç®€ä½“ä¸­æ–‡</a>
</p>

---

## Why This Exists

Modern LLMs excel at linguistic reasoning but consistently fail at tasks requiring **embodied spatial understanding** â€” the kind of intuition any physical agent acquires trivially through interaction with the real world.

Consider a simple request: *"Navigate from A1 to E5, avoiding walls."* A human child solves this instantly. State-of-the-art LLMs routinely generate paths that **walk through walls**, **teleport across obstacles**, or **exit the grid entirely** â€” violations that are physically impossible but textually plausible.

**Tractatus-Eval** quantifies this gap. Inspired by Wittgenstein's *Tractatus Logico-Philosophicus* â€” *"The limits of my language mean the limits of my world"* â€” this benchmark asks: **what are the limits of a world built entirely from text?**

## What It Measures

Each evaluation sample presents a **5Ã—5 grid navigation problem** with:

- A **start position** and **goal position**
- **3 impassable obstacles** (walls)
- An **ASCII map** for visual grounding
- **4 multiple-choice answers** (1 correct, 3 distractors)

The ground-truth shortest path is computed via **A\* search**. Distractors are specifically designed to exploit embodied cognition blindspots:

| Distractor Strategy | What It Tests |
|---|---|
| **Wall Teleportation** | Straight-line path ignoring obstacles â€” tests if the model treats `#` as truly impassable |
| **Random Walk** | Same-length random direction sequence â€” tests if the model actually traces the path |
| **Reversed Path** | Correct path played backwards â€” tests directional coherence |
| **Off-by-One Mutation** | Single-direction swap in the correct path â€” tests fine-grained spatial tracking |

> [!IMPORTANT]
> Every distractor candidate is validated through a **physics-engine playback** step before acceptance. Candidates that are secretly valid alternate paths (reach the goal without hitting any wall or boundary) are **automatically discarded**, preventing data contamination where correct answers would be scored as wrong. See [Data Integrity](#data-integrity) below.

## Sample Prompt

```
You are navigating a 5Ã—5 grid. Rows are labeled Aâ€“E (top to bottom),
columns 1â€“5 (left to right). You can move one step at a time: up, down,
left, or right. You CANNOT move diagonally, move outside the grid
boundaries, or pass through obstacle cells.

Grid map:
  1 2 3 4 5
A # . . E .
B . . . # .
C . . . . .
D . . . . .
E S . . # .

Start: E1  |  Goal: A4  |  Obstacles (impassable): A1, B4, E4

What is the shortest valid path from E1 to A4? Give your answer as a
comma-separated list of directions (up/down/left/right).
```

## Quick Start

### Generate the Dataset

```bash
# Default: 1000 samples, seed=42
python3 scripts/generate_spatial_eval.py

# Custom configuration
python3 scripts/generate_spatial_eval.py \
  --num-samples 2000 \
  --seed 123 \
  --output data/custom_eval.jsonl
```

**No dependencies required** â€” the generator uses only the Python standard library.

### Run Evaluation with lm-evaluation-harness

```bash
pip install lm-eval

lm_eval --model hf \
        --model_args pretrained=meta-llama/Llama-3-8B \
        --tasks spatial_embodied_logic \
        --include_path ./tasks \
        --batch_size 16
```

## Project Structure

```
tractatus_eval/
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_spatial_eval.py   # Data generator (A* + distractors + JSONL)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spatial_embodied_logic.jsonl  # Pre-generated 1000-sample dataset
â””â”€â”€ tasks/
    â””â”€â”€ spatial_embodied_logic.yaml   # EleutherAI lm-eval-harness task config
```

## Dataset Statistics

| Metric | Value |
|---|---|
| Total samples | 1,000 |
| Grid size | 5 Ã— 5 |
| Obstacles per grid | 3 |
| Choices per question | 4 (1 correct + 3 distractors) |
| Avg shortest path | 3.5 steps |
| Path length range | 1 â€“ 9 steps |
| Deduplication | SHA-256 fingerprint |
| Distractor validation | Physics-engine playback (0% contamination) |
| Generation efficiency | ~99% (1,006 attempts for 1,000 samples) |

## Baseline Results

Evaluated using [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) (0-shot, multiple choice). All runs on Apple M5 (24GB, MPS).

| Model | Params | acc | acc_norm | Time |
|---|---|---|---|---|
| EleutherAI/pythia-410m | 410M | 0.126 Â±0.011 | 0.150 Â±0.011 | 1m17s |
| EleutherAI/pythia-1.4b | 1.4B | 0.169 Â±0.012 | 0.195 Â±0.013 | 2m57s |
| EleutherAI/pythia-2.8b | 2.8B | 0.188 Â±0.012 | 0.191 Â±0.012 | 5m41s |
| TinyLlama-1.1B-Chat | 1.1B | 0.226 Â±0.013 | 0.213 Â±0.013 | 3m39s |
| microsoft/phi-2 | 2.7B | **0.322 Â±0.015** | **0.306 Â±0.015** | 6m46s |
| *Random baseline* | â€” | *0.250* | *â€”* | â€” |

> [!NOTE]
> **Key findings:** (1) A clear **scaling trend** exists within the Pythia family: 410M â†’ 1.4B â†’ 2.8B shows monotonic improvement, yet all remain **below random chance** (25%). (2) **Phi-2** is the only model that exceeds random chance, likely due to its code/math-heavy training mix. (3) Even the best-performing model (Phi-2) only reaches 32.2% â€” far from ceiling â€” confirming that embodied spatial reasoning remains genuinely hard for text-only LLMs.

### Expanded Embodied Tasks (New in v0.2!)

We have expanded the benchmark with 5 additional tasks that test distinct physical constraints. Evaluated with `EleutherAI/pythia-410m` (0-shot):

| Task | Physics Tested | Pythia-410m Accuracy |
|---|---|---|
| **Key-Lock Puzzles** | State dependency (keys must be gathered before doors) | 11.7% (deeply below random) |
| **Object Stacking** | Gravity, structural integrity, center-of-mass support | 21.4% (below random) |
| **Container Filling** | Volume, pouring transfers, capacity clipping (overflow) | 46.1% |
| **Circuit Connectivity** | Electrical path tracing and strict topological loops | 49.9% |
| **Collision Prediction** | Temporal extrapolation, object trajectories, spatial intersection | 50.0% |

## How It Works Under the Hood

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Generator Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Random  â”‚   A*     â”‚  Distractor  â”‚  Physics    â”‚  EleutherAI    â”‚
â”‚  Grid    â”‚  Search  â”‚  Engine      â”‚  Playback   â”‚  JSONL         â”‚
â”‚  Layout  â”‚  (path)  â”‚  (4 strats)  â”‚  Validator  â”‚  Assembly      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start,   â”‚ shortest â”‚ teleport,    â”‚ simulate    â”‚ {query,        â”‚
â”‚ end,     â”‚ valid    â”‚ random,      â”‚ each path â†’ â”‚  choices,      â”‚
â”‚ walls    â”‚ path     â”‚ reversed,    â”‚ reject if   â”‚  gold}         â”‚
â”‚          â”‚          â”‚ mutated      â”‚ valid alt   â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ reject if no valid path, duplicate, or contaminated â†“
                           retry loop
```

1. **Grid Generation** â€” Randomly places start, goal, and 3 obstacles on a 5Ã—5 grid
2. **A\* Pathfinding** â€” Computes the optimal path using Manhattan distance heuristic; unsolvable grids are discarded
3. **Prompt Rendering** â€” Converts the grid into a natural-language prompt with ASCII visualization
4. **Distractor Generation** â€” Creates candidates using 4 cognitively-targeted strategies
5. **Physics-Engine Validation** â€” Each distractor is simulated step-by-step on the grid; alternate valid paths are rejected
6. **Deduplication** â€” SHA-256 fingerprinting ensures no duplicate scenarios
7. **JSONL Output** â€” Assembles everything into EleutherAI-compatible format

## Data Integrity

Benchmark datasets are only as trustworthy as their answer keys. A naive distractor engine can accidentally generate **alternate valid paths** â€” paths that differ from the A\*-computed answer but still legally reach the goal without violating any physical constraint. If scored as "wrong," these contaminate the benchmark by **penalizing models that reason correctly**.

Tractatus-Eval solves this with a **physics-engine playback validator** (`simulate_path`). Before a distractor is accepted, the engine walks it step-by-step on the actual grid. A candidate is accepted as a valid distractor **only if** it satisfies at least one of:

- ğŸ’¥ **Hits a wall** â€” collides with an obstacle cell
- ğŸš« **Goes out of bounds** â€” steps outside the grid boundary
- ğŸ¯ **Misses the goal** â€” ends on a cell that is not the target

If a candidate passes all physical checks and reaches the goal, it is silently discarded as an alternate valid path.

```
CONTAMINATION AUDIT (seed=42, n=1000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total samples:            1,000
Total distractors:        3,000
Contaminated distractors: 0
Contamination rate:       0.0000%
âœ… ZERO contamination
```

## CLI Reference

```
usage: generate_spatial_eval.py [-h] [-n NUM_SAMPLES] [-o OUTPUT] [--seed SEED]

Arguments:
  -n, --num-samples   Number of samples to generate (default: 1000)
  -o, --output        Output JSONL path (default: data/spatial_embodied_logic.jsonl)
  --seed              Random seed for reproducibility (default: 42)
```

## Theoretical Background

This benchmark operationalizes a core insight from Wittgenstein's philosophy of language:

> *"Whereof one cannot speak, thereof one must be silent."*
> â€” *Tractatus Logico-Philosophicus*, Proposition 7

A text-only LLM has never experienced a wall. It has no **sensorimotor grounding** for the concept of "impassable." It can only pattern-match the *word* "obstacle" against its training distribution. Tractatus-Eval measures the **engineering cost of this philosophical gap** â€” and provides the data foundation for closing it via preference-based alignment (DPO) or external guardrails (NeMo Guardrails).

## Related Work

- **Project Daedalus** â€” Uses the failure modes exposed by Tractatus-Eval to build DPO training pairs (correct path vs. wall-phasing hallucination), empirically demonstrating the effectiveness of external guardrails over pure text fine-tuning.
- **NeMo Guardrails Integration** â€” Production deployment of deterministic spatial validation as a Colang-scripted guardrail, bypassing the model's embodied cognition gap entirely.

## License

MIT

---

<h1 align="center">ğŸ›ï¸ Project Tractatus-Eval</h1>
<p align="center"><em>å¤§è¯­è¨€æ¨¡å‹ç©ºé—´å…·èº«é€»è¾‘è¯„ä¼°åŸºå‡†</em></p>

---

## ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªé¡¹ç›®

ç°ä»£å¤§è¯­è¨€æ¨¡å‹ (LLM) åœ¨è¯­è¨€æ¨ç†ä¸Šè¡¨ç°å“è¶Šï¼Œå´åœ¨éœ€è¦**å…·èº«ç©ºé—´ç†è§£**çš„ä»»åŠ¡ä¸Šå±¡å±¡ç¿»è½¦â€”â€”è€Œè¿™ç§ç†è§£ï¼Œæ˜¯ä»»ä½•åœ¨ç‰©ç†ä¸–ç•Œä¸­è¡ŒåŠ¨è¿‡çš„æ™ºèƒ½ä½“éƒ½èƒ½è½»è€Œæ˜“ä¸¾ä¹ å¾—çš„ç›´è§‰ã€‚

ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼š*"ä» A1 å¯¼èˆªåˆ° E5ï¼Œé¿å¼€å¢™å£ã€‚"* ä¸€ä¸ªå°å­©å­éƒ½èƒ½ç¬é—´è§£å†³ã€‚ç„¶è€Œï¼Œæœ€å‰æ²¿çš„ LLM ç”Ÿæˆçš„è·¯å¾„å´ç»å¸¸**ç©¿å¢™è€Œè¿‡**ã€**ç¬ç§»è·¨è¶Šéšœç¢ç‰©**ã€ç”šè‡³**ç›´æ¥èµ°å‡ºç½‘æ ¼è¾¹ç•Œ**â€”â€”è¿™äº›è¡Œä¸ºåœ¨ç‰©ç†ä¸Šä¸å¯èƒ½å‘ç”Ÿï¼Œä½†ä»çº¯æ–‡æœ¬è§’åº¦çœ‹å´æ˜¯"åˆç†"çš„ã€‚

**Tractatus-Eval** é‡åŒ–äº†è¿™ä¸ªé¸¿æ²Ÿã€‚çµæ„Ÿæºè‡ªç»´ç‰¹æ ¹æ–¯å¦çš„ã€Šé€»è¾‘å“²å­¦è®ºã€‹ï¼ˆ*Tractatus Logico-Philosophicus*ï¼‰â€”â€”*"æˆ‘çš„è¯­è¨€çš„ç•Œé™æ„å‘³ç€æˆ‘çš„ä¸–ç•Œçš„ç•Œé™"*â€”â€”è¿™ä¸ªåŸºå‡†è¿½é—®ï¼š**ä¸€ä¸ªå®Œå…¨ç”±æ–‡æœ¬æ„å»ºçš„ä¸–ç•Œï¼Œå…¶æé™åœ¨å“ªé‡Œï¼Ÿ**

## æµ‹é‡ä»€ä¹ˆ

æ¯ä¸ªè¯„ä¼°æ ·æœ¬å‘ˆç°ä¸€ä¸ª **5Ã—5 ç½‘æ ¼å¯¼èˆªé—®é¢˜**ï¼š

- ä¸€ä¸ª**èµ·ç‚¹**å’Œä¸€ä¸ª**ç»ˆç‚¹**
- **3 ä¸ªä¸å¯ç©¿è¶Šçš„éšœç¢ç‰©**ï¼ˆå¢™å£ï¼‰
- ä¸€å¼  **ASCII åœ°å›¾**ç”¨äºè§†è§‰å®šä½
- **4 ä¸ªé€‰æ‹©é¡¹**ï¼ˆ1 ä¸ªæ­£ç¡® + 3 ä¸ªå¹²æ‰°é¡¹ï¼‰

çœŸå€¼æœ€çŸ­è·¯å¾„ç”± **A\* æœç´¢ç®—æ³•**è®¡ç®—ã€‚å¹²æ‰°é¡¹ä¸“é—¨è®¾è®¡ç”¨äºåˆ©ç”¨å…·èº«è®¤çŸ¥çš„ç›²åŒºï¼š

| å¹²æ‰°ç­–ç•¥ | æµ‹è¯•å†…å®¹ |
|---|---|
| **ç©¿å¢™ç›´çº¿** | æ— è§†éšœç¢ç‰©çš„ç›´çº¿è·¯å¾„â€”â€”æµ‹è¯•æ¨¡å‹æ˜¯å¦å°† `#` è§†ä¸ºçœŸæ­£ä¸å¯ç©¿è¶Š |
| **éšæœºæ¼«æ­¥** | ç­‰é•¿åº¦éšæœºæ–¹å‘åºåˆ—â€”â€”æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸæ­£è¿½è¸ªäº†è·¯å¾„ |
| **åå‘è·¯å¾„** | æ­£ç¡®è·¯å¾„çš„é€†åºâ€”â€”æµ‹è¯•æ–¹å‘ä¸€è‡´æ€§ |
| **å•æ­¥çªå˜** | åœ¨æ­£ç¡®è·¯å¾„ä¸­æ›¿æ¢ä¸€ä¸ªæ–¹å‘â€”â€”æµ‹è¯•ç»†ç²’åº¦ç©ºé—´è¿½è¸ªèƒ½åŠ› |

> [!IMPORTANT]
> æ¯ä¸ªå¹²æ‰°é¡¹å€™é€‰åœ¨è¢«æ¥å—ä¹‹å‰ï¼Œéƒ½ä¼šé€šè¿‡**ç‰©ç†å¼•æ“å›æ”¾**æ­¥éª¤è¿›è¡ŒéªŒè¯ã€‚é‚£äº›æœ¬è´¨ä¸Šæ˜¯æœ‰æ•ˆæ›¿ä»£è·¯å¾„çš„å€™é€‰ï¼ˆæˆåŠŸåˆ°è¾¾ç»ˆç‚¹ä¸”æœªç¢°æ’ä»»ä½•å¢™å£æˆ–è¾¹ç•Œï¼‰ä¼šè¢«**è‡ªåŠ¨ä¸¢å¼ƒ**ï¼Œé˜²æ­¢æ•°æ®æ±¡æŸ“ï¼ˆå³æŠŠæ­£ç¡®ç­”æ¡ˆåˆ¤ä¸ºé”™è¯¯ï¼‰ã€‚è¯¦è§ä¸‹æ–¹[æ•°æ®å®Œæ•´æ€§](#æ•°æ®å®Œæ•´æ€§)ã€‚

## å¿«é€Ÿå¼€å§‹

### ç”Ÿæˆæ•°æ®é›†

```bash
# é»˜è®¤ï¼š1000 æ¡æ ·æœ¬ï¼Œseed=42
python3 scripts/generate_spatial_eval.py

# è‡ªå®šä¹‰é…ç½®
python3 scripts/generate_spatial_eval.py \
  --num-samples 2000 \
  --seed 123 \
  --output data/custom_eval.jsonl
```

**æ— éœ€ä»»ä½•ä¾èµ–** â€”â€”ç”Ÿæˆå™¨ä»…ä½¿ç”¨ Python æ ‡å‡†åº“ã€‚

### ä½¿ç”¨ lm-evaluation-harness è¿è¡Œè¯„ä¼°

```bash
pip install lm-eval

lm_eval --model hf \
        --model_args pretrained=meta-llama/Llama-3-8B \
        --tasks spatial_embodied_logic \
        --include_path ./tasks \
        --batch_size 16
```

## æ•°æ®é›†ç»Ÿè®¡

| æŒ‡æ ‡ | å€¼ |
|---|---|
| æ€»æ ·æœ¬æ•° | 1,000 |
| ç½‘æ ¼å°ºå¯¸ | 5 Ã— 5 |
| æ¯ä¸ªç½‘æ ¼éšœç¢ç‰©æ•° | 3 |
| æ¯é¢˜é€‰é¡¹æ•° | 4ï¼ˆ1 æ­£ç¡® + 3 å¹²æ‰°ï¼‰ |
| å¹³å‡æœ€çŸ­è·¯å¾„ | 3.5 æ­¥ |
| è·¯å¾„é•¿åº¦èŒƒå›´ | 1 â€“ 9 æ­¥ |
| å»é‡æœºåˆ¶ | SHA-256 æŒ‡çº¹ |
| å¹²æ‰°é¡¹éªŒè¯ | ç‰©ç†å¼•æ“å›æ”¾ï¼ˆ0% æ±¡æŸ“ç‡ï¼‰ |
| ç”Ÿæˆæ•ˆç‡ | ~99%ï¼ˆ1,006 æ¬¡å°è¯•ç”Ÿæˆ 1,000 æ¡ï¼‰ |

## åŸºçº¿è¯„ä¼°ç»“æœ

ä½¿ç”¨ [EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) è¯„ä¼°ï¼ˆ0-shotï¼Œå¤šé€‰é¢˜ï¼‰ã€‚æ‰€æœ‰æµ‹è¯•åœ¨ Apple M5ï¼ˆ24GB, MPSï¼‰ä¸Šå®Œæˆã€‚

| æ¨¡å‹ | å‚æ•°é‡ | acc | acc_norm | è€—æ—¶ |
|---|---|---|---|---|
| EleutherAI/pythia-410m | 410M | 0.126 Â±0.011 | 0.150 Â±0.011 | 1m17s |
| EleutherAI/pythia-1.4b | 1.4B | 0.169 Â±0.012 | 0.195 Â±0.013 | 2m57s |
| EleutherAI/pythia-2.8b | 2.8B | 0.188 Â±0.012 | 0.191 Â±0.012 | 5m41s |
| TinyLlama-1.1B-Chat | 1.1B | 0.226 Â±0.013 | 0.213 Â±0.013 | 3m39s |
| microsoft/phi-2 | 2.7B | **0.322 Â±0.015** | **0.306 Â±0.015** | 6m46s |
| *éšæœºåŸºçº¿* | â€” | *0.250* | *â€”* | â€” |

> [!NOTE]
> **å…³é”®å‘ç°ï¼š** (1) Pythia å®¶æ—å†…å­˜åœ¨æ¸…æ™°çš„**æ‰©å±•è¶‹åŠ¿**ï¼š410M â†’ 1.4B â†’ 2.8B å‡†ç¡®ç‡å•è°ƒé€’å¢ï¼Œä½†å…¨éƒ¨**ä½äºéšæœºçŒœæµ‹åŸºçº¿**ï¼ˆ25%ï¼‰ã€‚ (2) **Phi-2** æ˜¯å”¯ä¸€è¶…è¿‡éšæœºåŸºçº¿çš„æ¨¡å‹ï¼Œå¯èƒ½å¾—ç›Šäºå…¶ä»£ç /æ•°å­¦å¯†é›†çš„è®­ç»ƒæ•°æ®ã€‚ (3) å³ä½¿è¡¨ç°æœ€å¥½çš„ Phi-2 ä¹Ÿä»…è¾¾åˆ° 32.2%â€”â€”è¿œæœªåˆ°å¤©èŠ±æ¿â€”â€”è¯å®äº†å…·èº«ç©ºé—´æ¨ç†å¯¹çº¯æ–‡æœ¬ LLM ä»ç„¶æ˜¯çœŸæ­£çš„éš¾é¢˜ã€‚

### æ‰©å±•çš„å…·èº«è®¤çŸ¥ä»»åŠ¡ (v0.2 æ–°å¢ï¼)

æˆ‘ä»¬åœ¨åŸºå‡†ä¸­æ–°å¢äº† 5 ä¸ªæµ‹è¯•ä¸åŒç‰©ç†çº¦æŸçš„ä»»åŠ¡ã€‚ä½¿ç”¨ `EleutherAI/pythia-410m` (0-shot) çš„è¯„ä¼°ç»“æœå¦‚ä¸‹ï¼š

| ä»»åŠ¡ | æµ‹è¯•çš„ç‰©ç†çº¦æŸ | Pythia-410m å‡†ç¡®ç‡ |
|---|---|---|
| **é’¥åŒ™-é”è°œé¢˜ (Key-Lock Puzzles)** | çŠ¶æ€ä¾èµ–ï¼ˆå¿…é¡»å…ˆæ‹¾å–é’¥åŒ™æ‰èƒ½å¼€é—¨ï¼‰ | 11.7% (è¿œä½äºéšæœºçŒœæµ‹) |
| **ç‰©ä½“å †å  (Object Stacking)** | é‡åŠ›ã€ç»“æ„å®Œæ•´æ€§ã€é‡å¿ƒæ”¯æ’‘ | 21.4% (ä½äºéšæœºçŒœæµ‹) |
| **å®¹å™¨è£…æ°´ (Container Filling)** | å®¹é‡ã€å€¾å€’è½¬ç§»ã€æº¢å‡ºé™åˆ¶ | 46.1% |
| **ç”µè·¯è¿é€šæ€§ (Circuit Connectivity)** | ç”µè·¯è¿½è¸ªä¸ä¸¥æ ¼çš„æ‹“æ‰‘å›è·¯ | 49.9% |
| **ç¢°æ’é¢„æµ‹ (Collision Prediction)** | æ—¶é—´å¤–æ¨ã€ç‰©ä½“è½¨è¿¹ã€ç©ºé—´ç›¸äº¤ | 50.0% |

## å·¥ä½œåŸç†

1. **ç½‘æ ¼ç”Ÿæˆ** â€” åœ¨ 5Ã—5 ç½‘æ ¼ä¸Šéšæœºæ”¾ç½®èµ·ç‚¹ã€ç»ˆç‚¹å’Œ 3 ä¸ªéšœç¢ç‰©
2. **A\* å¯»è·¯** â€” ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»å¯å‘å¼è®¡ç®—æœ€ä¼˜è·¯å¾„ï¼›æ— è§£ç½‘æ ¼è¢«ä¸¢å¼ƒ
3. **Prompt æ¸²æŸ“** â€” å°†ç½‘æ ¼è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æç¤ºè¯ï¼Œé™„å¸¦ ASCII å¯è§†åŒ–
4. **å¹²æ‰°é¡¹ç”Ÿæˆ** â€” ä½¿ç”¨ 4 ç§è®¤çŸ¥é¶å‘ç­–ç•¥ç”Ÿæˆå€™é€‰å¹²æ‰°é¡¹
5. **ç‰©ç†å¼•æ“éªŒè¯** â€” æ¯ä¸ªå¹²æ‰°é¡¹åœ¨ç½‘æ ¼ä¸Šé€æ­¥æ¨¡æ‹Ÿï¼›æœ‰æ•ˆæ›¿ä»£è·¯å¾„è¢«æ‹’ç»
6. **å»é‡** â€” SHA-256 æŒ‡çº¹ç¡®ä¿æ— é‡å¤åœºæ™¯
7. **JSONL è¾“å‡º** â€” ç»„è£…ä¸º EleutherAI å…¼å®¹æ ¼å¼

## æ•°æ®å®Œæ•´æ€§

åŸºå‡†æ•°æ®é›†çš„å¯ä¿¡åº¦å–å†³äºå…¶ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚ä¸€ä¸ªå¤©çœŸçš„å¹²æ‰°é¡¹å¼•æ“å¯èƒ½ä¼šæ„å¤–ç”Ÿæˆ**æœ‰æ•ˆæ›¿ä»£è·¯å¾„**â€”â€”ä¸ A\* è®¡ç®—çš„ç­”æ¡ˆä¸åŒï¼Œä½†ä»ç„¶åˆæ³•åˆ°è¾¾ç»ˆç‚¹ã€ä¸è¿åä»»ä½•ç‰©ç†çº¦æŸçš„è·¯å¾„ã€‚å¦‚æœå°†å…¶è¯„åˆ¤ä¸º"é”™è¯¯"ï¼Œå°±ä¼šæ±¡æŸ“åŸºå‡†ï¼Œ**æƒ©ç½šæ¨ç†æ­£ç¡®çš„æ¨¡å‹**ã€‚

Tractatus-Eval é€šè¿‡**ç‰©ç†å¼•æ“å›æ”¾éªŒè¯å™¨** (`simulate_path`) è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æ¥å—å¹²æ‰°é¡¹ä¹‹å‰ï¼Œå¼•æ“ä¼šåœ¨å®é™…ç½‘æ ¼ä¸Šé€æ­¥æ¨¡æ‹Ÿå…¶è¡Œèµ°è¿‡ç¨‹ã€‚å€™é€‰é¡¹**ä»…åœ¨**æ»¡è¶³ä»¥ä¸‹è‡³å°‘ä¸€ä¸ªæ¡ä»¶æ—¶æ‰ä¼šè¢«æ¥å—ä¸ºæœ‰æ•ˆå¹²æ‰°é¡¹ï¼š

- ğŸ’¥ **æ’å¢™** â€” ç¢°æ’éšœç¢ç‰©å•å…ƒæ ¼
- ğŸš« **å‡ºç•Œ** â€” èµ°å‡ºç½‘æ ¼è¾¹ç•Œ
- ğŸ¯ **æœªåˆ°è¾¾ç»ˆç‚¹** â€” ç»“æŸä½ç½®ä¸æ˜¯ç›®æ ‡å•å…ƒæ ¼

å¦‚æœå€™é€‰é¡¹é€šè¿‡æ‰€æœ‰ç‰©ç†æ£€æŸ¥å¹¶æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼Œåˆ™è¢«é™é»˜ä¸¢å¼ƒï¼ˆè§†ä¸ºæœ‰æ•ˆæ›¿ä»£è·¯å¾„ï¼‰ã€‚

```
æ•°æ®æ±¡æŸ“å®¡è®¡ (seed=42, n=1000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»æ ·æœ¬æ•°:              1,000
æ€»å¹²æ‰°é¡¹æ•°:            3,000
è¢«æ±¡æŸ“çš„å¹²æ‰°é¡¹:         0
æ±¡æŸ“ç‡:                0.0000%
âœ… é›¶æ±¡æŸ“
```

## ç†è®ºèƒŒæ™¯

æœ¬åŸºå‡†å°†ç»´ç‰¹æ ¹æ–¯å¦è¯­è¨€å“²å­¦ä¸­çš„æ ¸å¿ƒæ´è§å·¥ç¨‹åŒ–ï¼š

> *"å‡¡ä¸å¯è¨€è¯´è€…ï¼Œå¿…é¡»ä¿æŒæ²‰é»˜ã€‚"*
> â€” ã€Šé€»è¾‘å“²å­¦è®ºã€‹ï¼Œå‘½é¢˜ 7

ä¸€ä¸ªçº¯æ–‡æœ¬ LLM ä»æœª"ä½“éªŒ"è¿‡ä¸€å µå¢™ã€‚å®ƒå¯¹"ä¸å¯ç©¿è¶Š"è¿™ä¸ªæ¦‚å¿µæ²¡æœ‰ä»»ä½•**æ„Ÿè§‰è¿åŠ¨å±‚é¢çš„è½åœ° (Sensorimotor Grounding)**ï¼Œåªèƒ½å°†"éšœç¢ç‰©"è¿™ä¸ª*è¯*ä¸è®­ç»ƒåˆ†å¸ƒè¿›è¡Œæ¨¡å¼åŒ¹é…ã€‚Tractatus-Eval è¡¡é‡çš„æ˜¯**è¿™ä¸ªå“²å­¦é¸¿æ²Ÿçš„å·¥ç¨‹ä»£ä»·**â€”â€”å¹¶ä¸ºé€šè¿‡åå¥½å¯¹é½ (DPO) æˆ–å¤–éƒ¨æŠ¤æ  (NeMo Guardrails) å¼¥åˆå®ƒæä¾›æ•°æ®åŸºç¡€ã€‚

## ç›¸å…³é¡¹ç›®

- **Project Daedalus** â€” åˆ©ç”¨ Tractatus-Eval æš´éœ²çš„å¤±è´¥æ¨¡å¼æ„å»º DPO è®­ç»ƒå¯¹ï¼ˆæ­£ç¡®è·¯å¾„ vs. ç©¿å¢™å¹»è§‰ï¼‰ï¼Œå®è¯è®ºè¯äº†å¤–éƒ¨æŠ¤æ ä¼˜äºçº¯æ–‡æœ¬å¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚
- **NeMo Guardrails é›†æˆ** â€” å°†ç¡®å®šæ€§ç©ºé—´éªŒè¯éƒ¨ç½²ä¸º Colang è„šæœ¬æŠ¤æ ï¼Œå®Œå…¨ç»•è¿‡æ¨¡å‹çš„å…·èº«è®¤çŸ¥ç›²åŒºã€‚

## è®¸å¯è¯

MIT
